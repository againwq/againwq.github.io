<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>docker部署 on Xu Qianchao&#39;s Blog</title>
    <link>https://againwq.github.io/tags/docker%E9%83%A8%E7%BD%B2/</link>
    <description>Recent content in docker部署 on Xu Qianchao&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Tue, 12 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://againwq.github.io/tags/docker%E9%83%A8%E7%BD%B2/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>本地docker部署libretranslate翻译模型并采用CUDA加速</title>
      <link>https://againwq.github.io/posts/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/libretranslate%E7%9A%84docker%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://againwq.github.io/posts/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/libretranslate%E7%9A%84docker%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2/</guid>
      <description>Libtranslate的本地部署，并采用CUDA加速</description>
      <content:encoded><![CDATA[<h2 id="一libretanslate基本介绍">一、Libretanslate基本介绍</h2>
<p>   Libretanslate 是一个开源的，基于AI驱动的翻译软件，<a href="https://libretranslate.com/">官方网站</a>提供了在线的翻译功能，并且可以申请 api 密钥去调用 api 将翻译能力嵌入到我们自己的程序或者软件中。当然，<a href="https://github.com/LibreTranslate/LibreTranslate">官方的github</a>有详细的本地部署教程，如果有能力建议根据官方的 README 部署，本文是对可能遇到的一些问题的补充。本文采用的是 docker 部署，当然官方提供了直接通过 pip 包部署，读者可以根据自己的需求选择。效果图如下：</p>
<p><img loading="lazy" src="/images/Libretranslate%e7%bf%bb%e8%af%91%e6%95%88%e6%9e%9c.png" alt="libretranslate翻译效果"  />
</p>
<h2 id="二-docker-与-nvidia-docker-支持的前置条件">二. docker 与 nvidia docker 支持的前置条件</h2>
<p>   首先需要保证你的本地系统已经安装的 docker 环境，建议采用国内的源安装，比如清华源、阿里源等。同时建议配置docker镜像源防止由于网络问题无法拉取镜像。这里不提供安装配置命令，建议读者自行搜索相关资料。</br>
   使用 nvidia 的 docker 加速，需要读者本地拥有 nvidia 的显卡，并安装了显卡驱动。如果没有 nvidia 的显卡支持，可以跳过这一部分，进行 cpu 版本的本地部署。</br>
   首先要在自己的电脑上安装 nvidia docker 支持，参考的<a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">官方地址</a>。根据你的 linux 发行版复制粘贴命令就行了，debian 发行版的安装命令如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Shell" data-lang="Shell"><span class="line"><span class="cl"><span class="c1"># 添加 apt 源</span>
</span></span><span class="line"><span class="cl">curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey <span class="p">|</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg <span class="o">&amp;&amp;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list <span class="p">|</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>sed <span class="s1">&#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39;</span> <span class="p">|</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">sudo sed -i -e <span class="s1">&#39;/experimental/ s/^#//g&#39;</span> /etc/apt/sources.list.d/nvidia-container-toolkit.list
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 更新源</span>
</span></span><span class="line"><span class="cl">sudo apt-get update
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 安装nvidia-container-toolkit</span>
</span></span><span class="line"><span class="cl">sudo apt-get install -y nvidia-container-toolkit
</span></span></code></pre></div><p>   然后对 docker 进行配置：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Shell" data-lang="Shell"><span class="line"><span class="cl">sudo nvidia-ctk runtime configure --runtime<span class="o">=</span>docker
</span></span><span class="line"><span class="cl">sudo systemctl restart docker
</span></span></code></pre></div><h2 id="三docker版本的libretranslate的本地部署">三、docker版本的libretranslate的本地部署</h2>
<p>   接下来我们进行部署。首先，我们需要在 github 上下载源码：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Shell" data-lang="Shell"><span class="line"><span class="cl">git clone git@github.com:LibreTranslate/LibreTranslate.git
</span></span></code></pre></div><h3 id="1-cpu版本libretranslate的部署">1. cpu版本libretranslate的部署</h3>
<p>   cpu 版本部署非常简单，在源码根路径下执行下面的命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Shell" data-lang="Shell"><span class="line"><span class="cl">docker compose -f docker-compose.yml up -d --build
</span></span></code></pre></div><p>   这个命令会根据 docker/Dockerfile 文件构建 libretranslate 镜像并部署在本地的 5000 端口。- -build 代表重新构建镜像。第一次构建好镜像后下一次可以把 - -build命令去掉，每次创建容器都会从网络上下载指定的翻译模型，因此 5000 端口并不能立即访问，可以通过 <code>docker stats</code> 查看容器的网络 IO 状态判断翻译模型是否下载完毕。</p>
<h3 id="2-gpu版本libretranslate的部署">2. GPU版本libretranslate的部署</h3>
<p>   部署gpu版本的 libretranslate 命令与上面相同，只是将 yml 文件为 docker-compose.cuda.yml。在部署之前，最好修改 docker/cuda.Dockerfile 文件将 cuda 的相关环境变量导出，大概在文件末尾的位置添加下面几行：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Shell" data-lang="Shell"><span class="line"><span class="cl"><span class="c1"># Depending on your cuda install you may need to uncomment this line to allow the container to access the cuda libraries</span>
</span></span><span class="line"><span class="cl"><span class="c1"># See: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#post-installation-actions</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ENV LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib64</span>
</span></span><span class="line"><span class="cl">ENV <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/usr/local/cuda/lib64
</span></span><span class="line"><span class="cl">ENV <span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:/usr/local/cuda/bin
</span></span><span class="line"><span class="cl">ENV <span class="nv">CUDA_HOME</span><span class="o">=</span>/usr/local/cuda
</span></span></code></pre></div><p>   然后通过 <code>docker compose -f docker-compose.cuda.yml up -d --build</code>进行构建并部署即可。同样的，构建成功 docker 会启动在本地 5000 端口，每次创建容器都会从网络上下载指定的翻译模型。</p>
<h2 id="四可能遇到的问题">四、可能遇到的问题</h2>
<h3 id="1-构建镜像时apt和pip下载太慢">1. 构建镜像时apt和pip下载太慢</h3>
<p>   apt 和 pip 默认的官方源在国内访问不太稳定，可以在执行 <code>apt install</code>和 <code>pip install</code>之前更换镜像源为国内源，需要在对应的 Dockerfile中修改：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Shell" data-lang="Shell"><span class="line"><span class="cl"><span class="c1"># 更换apt源为清华源</span>
</span></span><span class="line"><span class="cl">RUN sed -i <span class="s1">&#39;s/archive.ubuntu.com/mirrors.tuna.tsinghua.edu.cn/g&#39;</span>  /etc/apt/sources.list
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#更换pip源为清华源，应该添加在Dockerfile中pip下载之后的位置</span>
</span></span><span class="line"><span class="cl">RUN pip3 config <span class="nb">set</span> global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># cpu版本中Dockerfile采用了venv</span>
</span></span><span class="line"><span class="cl">RUN ./venv/bin/pip config <span class="nb">set</span> global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple
</span></span></code></pre></div><h3 id="2-urlerrorconnectionrefusederror111-connection-refused在-docker-日志中大量出现无法下载翻译模型">2. (URLError(ConnectionRefusedError(111, &lsquo;Connection refused&rsquo;)),)在 docker 日志中大量出现，无法下载翻译模型</h3>
<p>   Libretranslate 是基于 <code>argos-translate</code> 这个开源翻译模型开发的项目，内部仍然调用的是 argos-translate，argos-tanslate 下载一个 <code>index.json</code> 文件，然后根据你指定的需要支持的语言从 index.json 中获取下载路径。index.json 默认下载到 <code>~/.local/cache/argos-translate</code> 翻译模型默认下载保存到当前路径下 <code>db/session</code> 下。我出现的问题是从 docker 内部是无法正常下载index.json的，但是主机是可以正常下载的，因此我采用的方法是手动下载 index.json 并将其放到 Libretranslate 源码根目录下，然后在我要构建的 Dockerfile 末尾修改为：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Shell" data-lang="Shell"><span class="line"><span class="cl"><span class="c1"># 将主机当前目录的index.json放到容器的/root/.local/cache/argos-translate/下</span>
</span></span><span class="line"><span class="cl">RUN mkdir -p /root/.local/cache/argos-translate/
</span></span><span class="line"><span class="cl">RUN mv ./index.json /root/.local/cache/argos-translate/
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">EXPOSE <span class="m">5000</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 原本的--host * 不知道为什么我会报错，这里改成 0.0.0.0就没有报错了</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 这里我只需要中英互译就可以了</span>
</span></span><span class="line"><span class="cl">ENTRYPOINT <span class="o">[</span><span class="s2">&#34;libretranslate&#34;</span>, <span class="s2">&#34;--host&#34;</span>, <span class="s2">&#34;0.0.0.0&#34;</span>, <span class="s2">&#34;--load-only&#34;</span>，<span class="s2">&#34;zh,en&#34;</span><span class="o">]</span>
</span></span></code></pre></div><p>   这是参考的解决方案的<a href="https://community.libretranslate.com/t/failing-to-download-from-cloudflare-with-connectionrefusederror/960">链接</a>，这里提供我保存的<a href="/common/index.json">index.json文件</a></p>
<h3 id="3-启用-cuda-加速后进行翻译出现内部错误">3. 启用 cuda 加速后进行翻译出现内部错误</h3>
<p>   建议先进入 docker 内部使用 python执行 <code>torch.cuda.is_available()</code> 查看 CUDA 是否成功支持。</br>
   这里我的问题是我的 <strong>nvidia 驱动版本是 debian12 默认下载的535, CUDA 版本最高支持到 12.2</strong>, 而且我本地的 CUDA 环境是 11.8。<strong>Libretranslate默认构建镜像的 CUDA 版本是 12.4</strong>, 版本过高导致 torch 调用硬件失败。解决的方法是将 Libretranslate 构建时采用的基础镜像从 <code>FROM nvidia/cuda:12.4.1-devel-ubuntu20.04</code> 更换为 <code>FROM nvidia/cuda:12.2.0-devel-ubuntu20.04</code>。 注意一定要是 12 版本以上的，我之前采用与本地相同的 11.8 启动仍然失败了，<code>torch.cuda.is_available()</code> 的返回值是 True，但是运行时会出现动态链接库找不到的问题，当前这个版本好像默认要求 CUDA 版本大于 12。</p>
<h3 id="参考文献">参考文献</h3>
<blockquote>
<p><a href="https://github.com/LibreTranslate/LibreTranslate">LibreTranslate的github</a> </br>
<a href="https://community.libretranslate.com/t/failing-to-download-from-cloudflare-with-connectionrefusederror/960">翻译模型下载失败参考解决方案</a> </br></p>
</blockquote>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
